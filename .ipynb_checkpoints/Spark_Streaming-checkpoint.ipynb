{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce794c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versão usada neste notebook: 3.9.12\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print(\"Versão usada neste notebook:\", python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e88b2277",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importando o findspark\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7da5255",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importações necessárias\n",
    "\n",
    "import pyspark\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "from pyspark.sql.functions import col, from_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44f3b3b",
   "metadata": {},
   "source": [
    "O Spark Streaming possui conexão com diversos frameworks e SGBDs, para o nosso projeto precisamos de um conector para o Apache Kafka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a27c2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usando o conector:\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995c3288",
   "metadata": {},
   "source": [
    "## Criando uma Sessão Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8ca6ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/19 08:38:48 WARN Utils: Your hostname, mor-Inspiron-3501 resolves to a loopback address: 127.0.1.1; using 192.168.15.175 instead (on interface wlp0s20f3)\n",
      "23/04/19 08:38:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/mor/.ivy2/cache\n",
      "The jars for the packages stored in: /home/mor/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-b674702c-d2d5-48dd-a1bb-7f1f6b8f0011;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.4.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.4.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.3.2 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.9.1 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.6 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.4.0/spark-sql-kafka-0-10_2.12-3.4.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.12;3.4.0!spark-sql-kafka-0-10_2.12.jar (706ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.4.0/spark-token-provider-kafka-0-10_2.12-3.4.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.4.0!spark-token-provider-kafka-0-10_2.12.jar (408ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.3.2/kafka-clients-3.3.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kafka#kafka-clients;3.3.2!kafka-clients.jar (2707ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar ...\n",
      "\t[SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.0!jsr305.jar (460ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-pool2;2.11.1!commons-pool2.jar (415ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.3.4/hadoop-client-runtime-3.3.4.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-client-runtime;3.3.4!hadoop-client-runtime.jar (5443ms)\n",
      "downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.8.0/lz4-java-1.8.0.jar ...\n",
      "\t[SUCCESSFUL ] org.lz4#lz4-java;1.8.0!lz4-java.jar (672ms)\n",
      "downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.9.1/snappy-java-1.1.9.1.jar ...\n",
      "\t[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.9.1!snappy-java.jar(bundle) (1485ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/2.0.6/slf4j-api-2.0.6.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;2.0.6!slf4j-api.jar (476ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.3.4/hadoop-client-api-3.3.4.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-client-api;3.3.4!hadoop-client-api.jar (7239ms)\n",
      "downloading https://repo1.maven.org/maven2/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar ...\n",
      "\t[SUCCESSFUL ] commons-logging#commons-logging;1.1.3!commons-logging.jar (337ms)\n",
      ":: resolution report :: resolve 20024ms :: artifacts dl 20364ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.3.2 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.4.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.4.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.6 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.9.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   11  |   11  |   11  |   0   ||   11  |   11  |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-b674702c-d2d5-48dd-a1bb-7f1f6b8f0011\n",
      "\tconfs: [default]\n",
      "\t11 artifacts copied, 0 already retrieved (56348kB/48ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/19 08:39:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/19 08:39:30 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/04/19 08:39:30 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "23/04/19 08:39:30 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "23/04/19 08:39:30 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "23/04/19 08:39:30 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "23/04/19 08:39:30 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\n",
      "23/04/19 08:39:30 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.\n"
     ]
    }
   ],
   "source": [
    "spark_session = SparkSession.builder.appName('Spark_Streaming').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1c0b19",
   "metadata": {},
   "source": [
    "## Leitura do Kafka Spark Structured Stream\n",
    "\n",
    "O que fazemos aqui é criar uma subscrição no tópico que tem o streaming de dados dos quais desejamos \"puxar\" esses dados\n",
    "Uma subscrição no tópico serve para receber tudo o que o tópico receber."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7448b84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aqui estabelecemos a conexão entre o Spark Streaming e o Apache Kafka\n",
    "\n",
    "df = spark_session \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "  .option(\"subscribe\", \"dsamp6\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025bb67c",
   "metadata": {},
   "source": [
    "## Definição do Schema da Fonte de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de927a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos o schema dos dados que desejamos capturar para análise (neste caso a temperatura de cada sensor)\n",
    "\n",
    "schema_dados_temp = StructType([StructField('leitura',\n",
    "                                           StructType([StructField('temperatura', DoubleType(), True)]),True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04c44751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agora definimos um schema global dos dados\n",
    "schema_dados_global = StructType([\n",
    "    StructField('id_sensor', StringType(), True),\n",
    "    StructField('id_equipamento', StringType(), True),\n",
    "    StructField('sensor', StringType(), True),\n",
    "    StructField('data_evento', StringType(), True),\n",
    "    StructField('padrao', schema_dados_temp, True),\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cd3558",
   "metadata": {},
   "source": [
    "## Parse da Fonte de Dados\n",
    "\n",
    "Neste ponto precisamos informar o Spark como ele deverá formatar os dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecf32e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capturamos cada linha de dado (cada valor) como string\n",
    "# Vale lembrar que o df é um conexão com o kafka (readStream)\n",
    "\n",
    "df_conversao = df.selectExpr('CAST(value AS STRING)') # Cada linha do arquivo de dados é um value e estamos convertendo \n",
    "                                                    # cada linha para string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6bf2a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse do formato JSON em dataframe\n",
    "\n",
    "#Pegamos os dados e o nomeamos de jsonData, convertemos então de json para o nosso schema de dados definido acima.\n",
    "#Depois selecionamos todos os dados.\n",
    "\n",
    "df_conversao = df_conversao.withColumn('jsonData', from_json(col('value'), schema_dados_global)).select('jsonData.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aad800b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_sensor: string (nullable = true)\n",
      " |-- id_equipamento: string (nullable = true)\n",
      " |-- sensor: string (nullable = true)\n",
      " |-- data_evento: string (nullable = true)\n",
      " |-- padrao: struct (nullable = true)\n",
      " |    |-- leitura: struct (nullable = true)\n",
      " |    |    |-- temperatura: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_conversao.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7362cb",
   "metadata": {},
   "source": [
    "## Preparando do Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "633055ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Não nos interessa todas as colunas mostradas no schema acima.\n",
    "# O precisamos será a coluna de sensor e de temperatura, para calcularmos a média por sensor.\n",
    "\n",
    "df_conversao_temp_sensor = df_conversao.select(col('padrao.leitura.temperatura').alias('temperatura'),\n",
    "                                              col('sensor'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e20cc07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- temperatura: double (nullable = true)\n",
      " |-- sensor: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_conversao_temp_sensor.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb33518",
   "metadata": {},
   "source": [
    "## Análise de Dados em Tempo Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0012462a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aqui temos o objeto que irá conter nossa análise, o cálculo da média das temperaturas por sensor\n",
    "\n",
    "df_media_temp_sensor = df_conversao_temp_sensor.groupBy('sensor').mean('temperatura')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04d73690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sensor: string (nullable = true)\n",
      " |-- avg(temperatura): double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_media_temp_sensor.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a627bc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renomeamos as colunas para simplificar nossa análise\n",
    "\n",
    "df_media_temp_sensor = df_media_temp_sensor.select(col('sensor').alias('sensor'),\n",
    "                                                  col('avg(temperatura)').alias('media_temp'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31cdfeba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sensor: string (nullable = true)\n",
      " |-- media_temp: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_media_temp_sensor.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a1cdd1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/20 08:54:34 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-b034a5da-ca5b-4493-8b20-255e316b7b4d. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/04/20 08:54:34 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+------+----------+\n",
      "|sensor|media_temp|\n",
      "+------+----------+\n",
      "+------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+--------+------------------+\n",
      "|  sensor|        media_temp|\n",
      "+--------+------------------+\n",
      "| sensor7|              79.0|\n",
      "|sensor34|              80.6|\n",
      "|sensor41|62.849999999999994|\n",
      "|sensor50|60.366666666666674|\n",
      "|sensor31| 36.94285714285714|\n",
      "|sensor38|             57.15|\n",
      "| sensor1|40.925000000000004|\n",
      "|sensor30|              72.2|\n",
      "|sensor10|             63.98|\n",
      "|sensor25|45.214285714285715|\n",
      "| sensor4| 73.51666666666667|\n",
      "| sensor5| 71.53333333333333|\n",
      "|sensor20|51.260000000000005|\n",
      "|sensor44|              43.8|\n",
      "| sensor8|             52.14|\n",
      "|sensor14|              48.0|\n",
      "|sensor24|15.666666666666666|\n",
      "|sensor43| 55.75714285714285|\n",
      "|sensor47|              52.9|\n",
      "|sensor26| 50.82000000000001|\n",
      "+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+--------+------------------+\n",
      "|  sensor|        media_temp|\n",
      "+--------+------------------+\n",
      "| sensor7| 80.65613577023493|\n",
      "|sensor34|  84.3587012987013|\n",
      "|sensor41| 64.55243619489562|\n",
      "|sensor50| 58.22434554973823|\n",
      "|sensor31| 37.58141809290952|\n",
      "|sensor38| 57.75738498789346|\n",
      "| sensor1| 38.24480198019803|\n",
      "|sensor30| 71.58542199488494|\n",
      "|sensor10| 62.67192513368979|\n",
      "|sensor25| 42.45537190082647|\n",
      "| sensor4|  72.8814621409922|\n",
      "| sensor5| 72.02843137254901|\n",
      "|sensor20|49.387439613526524|\n",
      "|sensor44| 40.02533936651584|\n",
      "|sensor19| 58.73832487309649|\n",
      "| sensor8| 52.12580645161293|\n",
      "|sensor14| 48.72765432098761|\n",
      "|sensor24|16.738000000000007|\n",
      "|sensor43|54.291542288557224|\n",
      "|sensor47| 53.70594059405943|\n",
      "+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Abrimos o streaming para análise de dados em tempo real, imprimindo o resultado no console.\n",
    "\n",
    "# Objeto que inicia a consulta ao streaming com formato de console\n",
    "\n",
    "query = df_media_temp_sensor.writeStream.outputMode('complete').format('console').start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b0c45baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/home/mor/anaconda3/lib/python3.9/socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Executamos a query do streaming e evitamos que o processo seja encerrado\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/streaming.py:107\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1320\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1313\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Executamos a query do streaming e evitamos que o processo seja encerrado\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3227a42e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Getting offsets from KafkaV2[Subscribe[dsamp6]]',\n",
       " 'isDataAvailable': False,\n",
       " 'isTriggerActive': True}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c05ac697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '79b41f04-096a-4334-a343-02882c1a9c4c',\n",
       " 'runId': '31d4d68b-7ca3-4f67-8de8-29d57658d9a7',\n",
       " 'name': None,\n",
       " 'timestamp': '2023-04-20T12:05:32.897Z',\n",
       " 'batchId': 3,\n",
       " 'numInputRows': 0,\n",
       " 'inputRowsPerSecond': 0.0,\n",
       " 'processedRowsPerSecond': 0.0,\n",
       " 'durationMs': {'latestOffset': 1, 'triggerExecution': 1},\n",
       " 'stateOperators': [{'operatorName': 'stateStoreSave',\n",
       "   'numRowsTotal': 50,\n",
       "   'numRowsUpdated': 0,\n",
       "   'allUpdatesTimeMs': 1100,\n",
       "   'numRowsRemoved': 0,\n",
       "   'allRemovalsTimeMs': 0,\n",
       "   'commitTimeMs': 13021,\n",
       "   'memoryUsedBytes': 101600,\n",
       "   'numRowsDroppedByWatermark': 0,\n",
       "   'numShufflePartitions': 200,\n",
       "   'numStateStoreInstances': 200,\n",
       "   'customMetrics': {'loadedMapCacheHitCount': 800,\n",
       "    'loadedMapCacheMissCount': 0,\n",
       "    'stateOnCurrentVersionSizeBytes': 30568}}],\n",
       " 'sources': [{'description': 'KafkaV2[Subscribe[dsamp6]]',\n",
       "   'startOffset': {'dsamp6': {'0': 40000}},\n",
       "   'endOffset': {'dsamp6': {'0': 40000}},\n",
       "   'latestOffset': {'dsamp6': {'0': 40000}},\n",
       "   'numInputRows': 0,\n",
       "   'inputRowsPerSecond': 0.0,\n",
       "   'processedRowsPerSecond': 0.0,\n",
       "   'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
       "    'maxOffsetsBehindLatest': '0',\n",
       "    'minOffsetsBehindLatest': '0'}}],\n",
       " 'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@5bd74f',\n",
       "  'numOutputRows': 0}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.lastProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "86c417cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "WriteToDataSourceV2 org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@28a9bcee, org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy$$Lambda$2405/0x00000001011a0040@68fabe64\n",
      "+- *(4) HashAggregate(keys=[sensor#37], functions=[avg(temperatura#58)])\n",
      "   +- StateStoreSave [sensor#37], state info [ checkpoint = file:/tmp/temporary-b034a5da-ca5b-4493-8b20-255e316b7b4d/state, runId = 31d4d68b-7ca3-4f67-8de8-29d57658d9a7, opId = 0, ver = 2, numPartitions = 200], Complete, 0, 2\n",
      "      +- *(3) HashAggregate(keys=[sensor#37], functions=[merge_avg(temperatura#58)])\n",
      "         +- StateStoreRestore [sensor#37], state info [ checkpoint = file:/tmp/temporary-b034a5da-ca5b-4493-8b20-255e316b7b4d/state, runId = 31d4d68b-7ca3-4f67-8de8-29d57658d9a7, opId = 0, ver = 2, numPartitions = 200], 2\n",
      "            +- *(2) HashAggregate(keys=[sensor#37], functions=[merge_avg(temperatura#58)])\n",
      "               +- Exchange hashpartitioning(sensor#37, 200), ENSURE_REQUIREMENTS, [plan_id=489]\n",
      "                  +- *(1) HashAggregate(keys=[sensor#37], functions=[partial_avg(temperatura#58)])\n",
      "                     +- Project [from_json(StructField(id_sensor,StringType,true), StructField(id_equipamento,StringType,true), StructField(sensor,StringType,true), StructField(data_evento,StringType,true), StructField(padrao,StructType(StructField(leitura,StructType(StructField(temperatura,DoubleType,true)),true)),true), cast(value#8 as string), Some(America/Sao_Paulo)).padrao.leitura.temperatura AS temperatura#58, from_json(StructField(id_sensor,StringType,true), StructField(id_equipamento,StringType,true), StructField(sensor,StringType,true), StructField(data_evento,StringType,true), StructField(padrao,StructType(StructField(leitura,StructType(StructField(temperatura,DoubleType,true)),true)),true), cast(value#8 as string), Some(America/Sao_Paulo)).sensor AS sensor#37]\n",
      "                        +- MicroBatchScan[key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13] class org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a72ee6",
   "metadata": {},
   "source": [
    "## Análise de Dados em Tempo Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "83e9ed7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/20 09:10:31 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-f5a21bb8-da14-42dd-a05d-476883b936de. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/04/20 09:10:31 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Objeto que inicia a consulta ao streaming com formato de memória (cria tabela temporária)\n",
    "# Esta tabela temporária é criada na memória\n",
    "\n",
    "query_memoria = df_media_temp_sensor \\\n",
    "    .writeStream \\\n",
    "    .queryName(\"memo\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7d66b0d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<pyspark.sql.streaming.StreamingQuery at 0x7fc5714c7880>,\n",
       " <pyspark.sql.streaming.StreamingQuery at 0x7fc571500c10>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Streams ativados\n",
    "spark_session.streams.active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "53ff2783",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+--------+------------------+\n",
      "|  sensor|        media_temp|\n",
      "+--------+------------------+\n",
      "| sensor7|  80.7078205128205|\n",
      "|sensor34|  84.3751282051282|\n",
      "|sensor41| 64.42126642771805|\n",
      "|sensor50| 58.38560606060607|\n",
      "|sensor31| 37.62222222222221|\n",
      "|sensor38| 57.88756345177665|\n",
      "| sensor1| 38.34196319018406|\n",
      "|sensor30| 71.62153236459712|\n",
      "|sensor10|62.702002670226925|\n",
      "|sensor25| 42.64750957854407|\n",
      "| sensor4| 73.00585034013606|\n",
      "| sensor5| 72.01032806804375|\n",
      "|sensor20| 49.43928571428568|\n",
      "|sensor44|39.950301568154416|\n",
      "|sensor19| 58.72225000000004|\n",
      "| sensor8| 52.10872817955112|\n",
      "|sensor14| 48.98612440191384|\n",
      "|sensor24| 16.68031980319803|\n",
      "|sensor43| 54.42487922705317|\n",
      "|sensor47|  53.5885101010101|\n",
      "+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 6\n",
      "-------------------------------------------\n",
      "+--------+------------------+\n",
      "|  sensor|        media_temp|\n",
      "+--------+------------------+\n",
      "| sensor7| 80.68432708688245|\n",
      "|sensor34| 84.38411614005123|\n",
      "|sensor41|  64.4394611727417|\n",
      "|sensor50| 58.37806943268418|\n",
      "|sensor31| 37.62614107883816|\n",
      "|sensor38| 57.92165820642979|\n",
      "| sensor1|38.312758620689664|\n",
      "|sensor30| 71.69095563139933|\n",
      "|sensor10| 62.71521926053305|\n",
      "|sensor25| 42.72114236999148|\n",
      "| sensor4| 73.05489177489177|\n",
      "| sensor5| 71.97064595257564|\n",
      "|sensor20|49.505354200988435|\n",
      "|sensor44|39.848497156783104|\n",
      "|sensor19|58.842004971002524|\n",
      "| sensor8| 52.13907172995782|\n",
      "|sensor14| 48.97538213998388|\n",
      "|sensor24| 16.67905004240882|\n",
      "|sensor43|  54.3588972431078|\n",
      "|sensor47| 53.50851979345956|\n",
      "+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 7\n",
      "-------------------------------------------\n",
      "+--------+------------------+\n",
      "|  sensor|        media_temp|\n",
      "+--------+------------------+\n",
      "| sensor7| 80.68095238095238|\n",
      "|sensor34| 84.38549488054606|\n",
      "|sensor41| 64.44264240506331|\n",
      "|sensor50|58.381802864363955|\n",
      "|sensor31|37.630737365368674|\n",
      "|sensor38| 57.92516891891893|\n",
      "| sensor1|38.314703353396396|\n",
      "|sensor30| 71.69558198810536|\n",
      "|sensor10| 62.71400343642606|\n",
      "|sensor25| 42.72189097103919|\n",
      "| sensor4|  73.0492666091458|\n",
      "| sensor5| 71.97765089722675|\n",
      "|sensor20| 49.51549180327866|\n",
      "|sensor44|39.850770478507705|\n",
      "|sensor19| 58.83887510339127|\n",
      "| sensor8| 52.14352941176472|\n",
      "|sensor14| 48.97443729903534|\n",
      "|sensor24|16.676949152542374|\n",
      "|sensor43|  54.3586666666667|\n",
      "|sensor47| 53.50935622317597|\n",
      "+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 8\n",
      "-------------------------------------------\n",
      "+--------+------------------+\n",
      "|  sensor|        media_temp|\n",
      "+--------+------------------+\n",
      "| sensor7|  80.7106329113924|\n",
      "|sensor34| 84.43331202046035|\n",
      "|sensor41| 64.50567765567767|\n",
      "|sensor50|58.435455124124765|\n",
      "|sensor31|37.703567035670346|\n",
      "|sensor38|57.920769230769224|\n",
      "| sensor1| 38.31147335423198|\n",
      "|sensor30| 71.67444089456869|\n",
      "|sensor10| 62.71627612412915|\n",
      "|sensor25|42.744812499999995|\n",
      "| sensor4| 73.06052295918367|\n",
      "| sensor5|  71.9385089340727|\n",
      "|sensor20| 49.52092739475285|\n",
      "|sensor44| 39.86604361370716|\n",
      "|sensor19| 58.85559655596558|\n",
      "| sensor8| 52.07428393524284|\n",
      "|sensor14| 49.02474226804122|\n",
      "|sensor24|16.711719253058597|\n",
      "|sensor43|54.432601880877776|\n",
      "|sensor47|53.452858958068624|\n",
      "+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vamos manter a query executando por algum tempo e aplicando SQL aos dados em tempo real\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "for i in range(30):\n",
    "    \n",
    "    spark_session.sql(\"select sensor, round(media_temp, 2) as media from memo where media_temp > 65\")\n",
    "    sleep(3)\n",
    "    \n",
    "query_memoria.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fca35b",
   "metadata": {},
   "source": [
    "## Fim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
